{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file extracts data from the eo-patches to produce pandas dataframes suitable for machine learning\n",
    "- The raw harvest data is not public and therefore not included in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point\n",
    "import matplotlib.pyplot as plt\n",
    "import pyproj\n",
    "import gzip\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from time import time\n",
    "from enum import Enum, auto\n",
    "\n",
    "from util_paths import SWEDEN\n",
    "from util_paths import HARVEST_17_20_AOI_FILE\n",
    "from util_paths import SENTINEL_1_11x11_SAR2SAR_V3_PATH\n",
    "from util_paths import SHARED_FOLDER_PATH\n",
    "from util_paths import UTM33N\n",
    "from util_paths import find_eo_patches, extract_eo_patch_data\n",
    "\n",
    "from utils_fvs_sampling_save import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = [2017, 2018, 2019, 2020]\n",
    "SENT_1 = SENTINEL_1_11x11_SAR2SAR_V3_PATH\n",
    "SENTINEL_1_PATHS = {year: os.path.join(SENT_1, str(year)) for year in YEARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2017: '/mimer/NOBACKUP/groups/snic2022-23-428/sentinel_1_data/despeckle_sar2sarV3_res_11m/2017',\n",
       " 2018: '/mimer/NOBACKUP/groups/snic2022-23-428/sentinel_1_data/despeckle_sar2sarV3_res_11m/2018',\n",
       " 2019: '/mimer/NOBACKUP/groups/snic2022-23-428/sentinel_1_data/despeckle_sar2sarV3_res_11m/2019',\n",
       " 2020: '/mimer/NOBACKUP/groups/snic2022-23-428/sentinel_1_data/despeckle_sar2sarV3_res_11m/2020'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTINEL_1_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2017: '/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/raw_data_12m/weather_topology_augmented/hostvete_weather_topology_augmented_2017.json',\n",
       " 2018: '/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/raw_data_12m/weather_topology_augmented/hostvete_weather_topology_augmented_2018.json',\n",
       " 2019: '/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/raw_data_12m/weather_topology_augmented/hostvete_weather_topology_augmented_2019.json',\n",
       " 2020: '/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/raw_data_12m/weather_topology_augmented/hostvete_weather_topology_augmented_2020.json'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HARVEST_DATA_FILES = {year: os.path.join(SHARED_FOLDER_PATH, \"hostvete\", f\"gridify_{year}_hv_mv.json\") for year in YEARS}\n",
    "#HARVEST_DATA_FILES = {year: os.path.join(SHARED_FOLDER_PATH, \"hostvete\", f\"hostvete_weather_topology_augmented_{year}.json\") for year in YEARS}\n",
    "HARVEST_DATA_FILES = {\n",
    "    year: os.path.join(SHARED_FOLDER_PATH, \n",
    "        \"raw_data_12m\", \n",
    "        f\"weather_topology_augmented\", \n",
    "        f\"hostvete_weather_topology_augmented_{year}.json\"\n",
    "    ) \n",
    "        for year in YEARS\n",
    "}\n",
    "HARVEST_DATA_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_data = gpd.read_file(HARVEST_DATA_FILES[2017]).to_crs(crs = UTM33N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for a single EO patch\n",
    "\n",
    "- Allows us to do basic checks and sample from a single patch\n",
    "- Only basic Nearest neighbour sampling implemented which is probably sufficient for 50m resolution, radial sampling requires a more sophisticated method because of the low resolution in satellite imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleType(Enum):\n",
    "    NearestPixel = auto()\n",
    "    Grid = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EoPatch:\n",
    "    \n",
    "    def __init__(self, year, patch_id, path):\n",
    "        self.year = year\n",
    "        self.patch_id = patch_id\n",
    "        self.bbox = gpd.read_file(os.path.join(path, \"bbox.geojson\")).loc[0].geometry\n",
    "        \n",
    "        with open(os.path.join(path, \"timestamp.json\"), 'r') as f:\n",
    "            self.dates = [\n",
    "                datetime.datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%S\") \n",
    "                    for ts in json.loads(f.read())\n",
    "            ]\n",
    "            \n",
    "        with open(os.path.join(path, \"data\", \"IW.npy\"), 'rb') as f:\n",
    "            # IW.npy = (time, longitude, latitude, [vh, vv])\n",
    "            self.VH_idx = 0\n",
    "            self.VV_idx = 1\n",
    "            self.radar_tensor = np.load(f)\n",
    "            \n",
    "        self.n_rows = self.radar_tensor.shape[1] # y range\n",
    "        self.n_cols = self.radar_tensor.shape[2] # x range\n",
    "        \n",
    "        #Verify that we have approximately the same resolution (meters/pixel) in both lat and lon\n",
    "        res_x = (self.bbox.bounds[2] - self.bbox.bounds[0])/self.n_cols\n",
    "        res_y = (self.bbox.bounds[3] - self.bbox.bounds[1])/self.n_rows\n",
    "        if round(res_x,0) == round(res_y,0):\n",
    "            self.res = round(res_x,0)\n",
    "        else:\n",
    "            print(f\"Resolution in latitude and longitude for EoPatch({self.patch_id})does not match\")\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"EoPatch({self.patch_id})\"\n",
    "\n",
    "    def _dates_in_week(week_nbr):\n",
    "        dates_in_week = [date for (date, w) in self.intersection_week if w == week]\n",
    "\n",
    "    def _find_indices_for_dates(self, dates):\n",
    "        return [i for (i, date_time) in enumerate(self.dates) if date_time.date() in dates]\n",
    "    \n",
    "    def contains(self, lon, lat):\n",
    "        \"\"\"Check if given point is inside patch.\"\"\"\n",
    "        return self.bbox.contains(Point(lon,lat))\n",
    "    \n",
    "    def coord_to_pixel(self, lon, lat):\n",
    "        \"\"\"Convert UTM33N coordinates to pixel coordinate in array (x,y).\"\"\"\n",
    "        relative_lon = lon - self.bbox.bounds[0]\n",
    "        relative_lat = lat - self.bbox.bounds[1]\n",
    "        x = np.floor(relative_lon / self.res).astype(int)\n",
    "        # Larger Lat -> Small Y, Small lat -> Large Y\n",
    "        relative_y = (self.n_rows - 1) - np.floor(relative_lat / self.res).astype(int)\n",
    "        y = max(relative_y, 0)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def _get_neighbours(self, x, y, i):\n",
    "        # clock wise, starting at 12\n",
    "        # We just assume that no point is exactly on the border for this to work ...\n",
    "        neighbours_idx = [\n",
    "            (x, y),\n",
    "            (x, y-1),   # north\n",
    "            (x+1, y-1), # north east\n",
    "            (x+1, y),   # east\n",
    "            (x+1, y+1), # south east\n",
    "            (x, y+1),   # south\n",
    "            (x-1, y+1), # south west\n",
    "            (x-1, y),   # west\n",
    "            (x-1, y-1), # north west\n",
    "        ]\n",
    "        \n",
    "        return np.array([self.radar_tensor[i, ny, nx, :] for nx, ny in neighbours_idx])\n",
    "        \n",
    "        \n",
    "    def _sample(self,x,y,dates_in_week):\n",
    "        for i in self._find_indices_for_dates(dates_in_week):\n",
    "            # First version\n",
    "            sample = self.radar_tensor[i,y,x,:]\n",
    "            if not np.isnan(sample).any():\n",
    "                return sample\n",
    "                \n",
    "        return np.array([np.nan,np.nan])\n",
    "    \n",
    "    def _sample_grid(self, x, y, dates_in_week):\n",
    "        for i in self._find_indices_for_dates(dates_in_week):\n",
    "            #including x, y point\n",
    "            neighbours = self._get_neighbours(x,y,i)\n",
    "            if not np.isnan(neighbours).any():\n",
    "                return neighbours\n",
    "            \n",
    "        return np.full((9, 2), np.nan)\n",
    "                \n",
    "    #Use nearest neighbor sampling for now\n",
    "    def sample(self, lon, lat, dates_in_week, sample_type):\n",
    "        if not self.contains(lon,lat):\n",
    "            print(\"Error: Point not inside patch bbox\")\n",
    "            return\n",
    "        x, y = self.coord_to_pixel(lon,lat)\n",
    "        \n",
    "        if sample_type == SampleType.NearestPixel:\n",
    "            return self._sample(x, y, dates_in_week)\n",
    "        \n",
    "        elif sample_type == SampleType.Grid:\n",
    "            return self._sample_grid(x, y, dates_in_week)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown sample type\", sample_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Class for all EO patches\n",
    "\n",
    "- Allows us to sample from all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EoPatches:\n",
    "    \n",
    "    def __init__(self, year, sent_1_path, sample_type=SampleType.NearestPixel):\n",
    "        self.year = year\n",
    "        patch_ids, paths = find_eo_patches(sent_1_path)\n",
    "        self.patches = [\n",
    "            EoPatch(year, patch_id, path) for patch_id, path in tqdm(zip(patch_ids, paths))\n",
    "        ]\n",
    "        self.sample_type = sample_type\n",
    "        self.sync_dates()\n",
    "        \n",
    "    def sync_dates(self):\n",
    "        self._find_intersecting_timestamps()\n",
    "        self._filter_eo_patches_dates()\n",
    "        self._check_contains_same_dates\n",
    "        \n",
    "        \n",
    "    def sample(self, lon, lat, week):\n",
    "        \"\"\"Get the pixel value from Sentinel-1 SAR or None.\"\"\"\n",
    "        for patch in self.patches:\n",
    "            if patch.contains(lon, lat):\n",
    "                dates_in_week = [date for (date, w) in self.intersection_week if w == week]\n",
    "                return patch.sample(lon,lat,dates_in_week, self.sample_type)\n",
    "        # print(f\"Error: Point {lat} {lon} is not contained in any Eo Patch\")\n",
    "    \n",
    "    def _filter_eo_patches_dates(self):\n",
    "        for patch in self.patches:\n",
    "            indices = []\n",
    "            for i, date in enumerate(patch.dates):\n",
    "                # remove this data from patch\n",
    "                if date.date() not in self.intersection:\n",
    "                    del patch.dates[i]\n",
    "                    indices.append(i)\n",
    "            patch.radar_tensor = np.delete(patch.radar_tensor, indices, axis=0)  \n",
    "    \n",
    "    def _find_intersecting_timestamps(self):\n",
    "        \"\"\"Find common dates for all Eo Patches.\"\"\"\n",
    "        sets = [\n",
    "            set([dt.date() for dt in patch.dates])\n",
    "                for patch in self.patches\n",
    "        ]\n",
    "        self.intersection = list(set.intersection(*sets))\n",
    "        self.intersection.sort()\n",
    "        self.intersection_week = [\n",
    "            (date, date.isocalendar().week)\n",
    "                for date in self.intersection\n",
    "        ]\n",
    "        self.first_week = min(self.intersection).isocalendar().week\n",
    "        self.last_week = max(self.intersection).isocalendar().week\n",
    "        \n",
    "    def _check_contains_same_dates(self):\n",
    "        sets = [\n",
    "            set([dt.date() for dt in patch.dates])\n",
    "                for patch in self.patches\n",
    "        ]\n",
    "        diff = set.difference(*sets, set(self.intersection))\n",
    "        print(diff)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices using the VV and VH radar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RVI(VH, VV):\n",
    "    \"\"\"Index defined from study. Seems to be inverted for our fields.\n",
    "    \n",
    "    Always > 0\n",
    "    \"\"\"\n",
    "    return VH / (VV+VH)\n",
    "\n",
    "def my_RVI(VH, VV):\n",
    "    \"\"\"Our own rvi, as we see better results for this.\n",
    "    \n",
    "    Always > 0\n",
    "    \"\"\"\n",
    "    return VV / (VV+VH)\n",
    "\n",
    "def R_VV_VH(VH, VV):\n",
    "    # Always > 0\n",
    "    return VV / VH\n",
    "\n",
    "def R_VH_VV(VH, VV):\n",
    "    # Always > 0\n",
    "    return VH / VV\n",
    "\n",
    "def my_index0(VH, VV):\n",
    "    # Can be negative\n",
    "    return (VH-VV) / (VH+VV)\n",
    "\n",
    "def my_index1(VH, VV):\n",
    "    # Can be negative\n",
    "    return (VH-VV) / VH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Grid of 1 dist neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_general(year, out_feather_name, folder, sample_type):\n",
    "    path = os.path.join(folder, out_feather_name)\n",
    "    if os.path.exists(path):\n",
    "        print(\"Already created this dataset, skipping ...\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading eo patches, {year}\")\n",
    "    start = time()\n",
    "    eo_patches = EoPatches(year, SENTINEL_1_PATHS[year], sample_type)\n",
    "    end = time()\n",
    "    print(f\"Elapsed time: {end-start}\")\n",
    "    \n",
    "    print(\"Loading harvest data\")\n",
    "    harvest_data = gpd.read_file(HARVEST_DATA_FILES[year]).to_crs(crs = UTM33N)\n",
    "    weeks = range(eo_patches.first_week, eo_patches.last_week)\n",
    "    columns = [\"harvest\", \"year\"]\n",
    "    \n",
    "    for week in weeks:\n",
    "        if sample_type == SampleType.Grid:\n",
    "            for n_name in neighbours_col_names:\n",
    "                columns += [\n",
    "                    f\"{n_name}_vh_week_{week}\",\n",
    "                    f\"{n_name}_vv_week_{week}\",\n",
    "                    f\"{n_name}_rvi_week_{week}\",\n",
    "                    f\"{n_name}_my_rvi_week_{week}\",\n",
    "                    f\"{n_name}_vh/vv_week_{week}\",\n",
    "                    f\"{n_name}_vv/vh_week_{week}\",\n",
    "                    f\"{n_name}_mi0_week_{week}\",\n",
    "                    f\"{n_name}_mi1_week_{week}\"\n",
    "                ]\n",
    "                \n",
    "        elif sample_type == SampleType.NearestPixel:\n",
    "            columns += [\n",
    "                f\"vh_week_{week}\",\n",
    "                f\"vv_week_{week}\",\n",
    "                f\"rvi_week_{week}\",\n",
    "                f\"my_rvi_week_{week}\",\n",
    "                f\"vh/vv_week_{week}\",\n",
    "                f\"vv/vh_week_{week}\",\n",
    "                f\"mi0_week_{week}\",\n",
    "                f\"mi1_week_{week}\"\n",
    "            ]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Unknown sample type\", sample_type)\n",
    "            \n",
    "    \n",
    "    def create_row(lon, lat, harvest):\n",
    "        row = [harvest, year]\n",
    "        \n",
    "        for week in weeks:\n",
    "            sample = eo_patches.sample(lon,lat,week) \n",
    "            if sample_type == SampleType.Grid:\n",
    "                for neighbour in sample:\n",
    "                    vh, vv = tuple(neighbour)\n",
    "                    row += [vh, vv, RVI(vh, vv), my_RVI(vh, vv), R_VH_VV(vh, vv), R_VV_VH(vh, vv), my_index0(vh, vv), my_index1(vh, vv)]\n",
    "                    \n",
    "            elif sample_type == SampleType.NearestPixel:\n",
    "                vh, vv = tuple(sample)\n",
    "                row += [vh, vv, RVI(vh, vv), my_RVI(vh, vv), R_VH_VV(vh, vv), R_VV_VH(vh, vv), my_index0(vh, vv), my_index1(vh, vv)]\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Unknown sample type\", sample_type)\n",
    "        return row\n",
    "    \n",
    "    print(\"Zipping data used form field\")\n",
    "    sample_data = zip(harvest_data.x.values, harvest_data.y.values, harvest_data.average_harvest)\n",
    "    \n",
    "    print(\"Creating dataframe\")\n",
    "    start = time()\n",
    "    df = pd.DataFrame(\n",
    "        [create_row(lon,lat,harvest) for (lon, lat, harvest) in tqdm(sample_data, position=0, leave=True)],\n",
    "        columns=columns\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"Elapsed time: {end-start}\")\n",
    "    \n",
    "    weather_df = harvest_data.loc[:, [col for col in harvest_data.columns if is_weather(col)]]\n",
    "    if sample_type == SampleType.Grid:\n",
    "        topology_df = harvest_data.loc[:, [col for col in harvest_data.columns if is_topology(col) or is_int_topology(col)]]\n",
    "        height_df = harvest_data.loc[:, [col for col in harvest_data.columns if is_height(col)]]\n",
    "\n",
    "    elif sample_type == SampleType.NearestPixel:\n",
    "        topology_df = harvest_data.loc[:, [col for col in harvest_data.columns if (is_topology(col) or is_int_topology(col)) and is_nearest_sampling(col)]]\n",
    "        height_df = harvest_data.loc[:, [col for col in harvest_data.columns if is_height(col) and is_nearest_sampling(col)]]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown sample type\", sample_type)\n",
    "    \n",
    "    df = pd.concat([df, weather_df, topology_df, height_df], axis=1)\n",
    "    print(f\"Drop NaN and save dataset at: {out_feather_name}\")\n",
    "    df = df.dropna().reset_index().drop([\"index\"], axis=1)\n",
    "    \n",
    "    df.to_feather(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HARVEST_DATA_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eo patches, 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:20,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 33.09646725654602\n",
      "Loading harvest data\n",
      "Zipping data used form field\n",
      "Creating dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64842it [03:58, 271.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 241.50645446777344\n",
      "Drop NaN and save dataset at: dataset_y_2017_res_11_sar2sar_v3.feather\n",
      "\n",
      "Loading eo patches, 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:47,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 61.6391818523407\n",
      "Loading harvest data\n",
      "Zipping data used form field\n",
      "Creating dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52455it [03:01, 288.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 183.93056535720825\n",
      "Drop NaN and save dataset at: dataset_y_2018_res_11_sar2sar_v3.feather\n",
      "\n",
      "Loading eo patches, 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:06, 11.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 79.15417551994324\n",
      "Loading harvest data\n",
      "Zipping data used form field\n",
      "Creating dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120500it [05:42, 351.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 347.92143964767456\n",
      "Drop NaN and save dataset at: dataset_y_2019_res_11_sar2sar_v3.feather\n",
      "\n",
      "Loading eo patches, 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:58,  9.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 72.97958397865295\n",
      "Loading harvest data\n",
      "Zipping data used form field\n",
      "Creating dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98528it [05:23, 305.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 327.5647096633911\n",
      "Drop NaN and save dataset at: dataset_y_2020_res_11_sar2sar_v3.feather\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder = \"/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/datasets/12m\"\n",
    "\n",
    "def run_grid():    \n",
    "    res = 11\n",
    "    sample_type = SampleType.NearestPixel\n",
    "    for year in YEARS:\n",
    "        out_feather_name = f\"dataset_y_{year}_res_{res}_sar2sar_v3.feather\"\n",
    "        create_dataset_general(year, out_feather_name, folder, sample_type)\n",
    "        print()\n",
    "        \n",
    "run_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_years(out_feather_name, folder):\n",
    "    paths = glob.glob(os.path.join(folder, '*.feather'))\n",
    "    paths.sort()\n",
    "    \n",
    "    cols = [pd.read_feather(path).columns for path in paths]\n",
    "\n",
    "    all_columns = set.union(*[set(col) for col in cols])\n",
    "    shared_columns = set.intersection(*[set(col)for col in cols])\n",
    "    not_shared_columns = list(set.difference(all_columns, shared_columns))\n",
    "    \n",
    "    #print(len(all_columns))\n",
    "    #for c in sorted(list(all_columns)):\n",
    "    #    print(c)\n",
    "        \n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_feather(path)\n",
    "        cols = df.columns\n",
    "        df = df.drop(\n",
    "            [col for col in cols if col in not_shared_columns],\n",
    "            axis=1\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df_all = pd.concat(dfs).dropna().reset_index().drop([\"index\"], axis=1)\n",
    "    \n",
    "    df_all.to_feather(os.path.join(SHARED_FOLDER_PATH, \"datasets\", out_feather_name))\n",
    "\n",
    "    return df_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_name = f\"dataset_y_{YEARS[0]}_{YEARS[-1]}_11m_hd.feather\"\n",
    "df_all = concat_years(df_all_name, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in sorted(df_all[df_all[\"year\"] == 2020].columns):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all[\"year\"] == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harvest\n",
      "year\n",
      "vh_week_14\n",
      "vv_week_14\n",
      "rvi_week_14\n",
      "my_rvi_week_14\n",
      "vh/vv_week_14\n",
      "vv/vh_week_14\n",
      "mi0_week_14\n",
      "mi1_week_14\n",
      "vh_week_15\n",
      "vv_week_15\n",
      "rvi_week_15\n",
      "my_rvi_week_15\n",
      "vh/vv_week_15\n",
      "vv/vh_week_15\n",
      "mi0_week_15\n",
      "mi1_week_15\n",
      "vh_week_16\n",
      "vv_week_16\n",
      "rvi_week_16\n",
      "my_rvi_week_16\n",
      "vh/vv_week_16\n",
      "vv/vh_week_16\n",
      "mi0_week_16\n",
      "mi1_week_16\n",
      "vh_week_17\n",
      "vv_week_17\n",
      "rvi_week_17\n",
      "my_rvi_week_17\n",
      "vh/vv_week_17\n",
      "vv/vh_week_17\n",
      "mi0_week_17\n",
      "mi1_week_17\n",
      "vh_week_18\n",
      "vv_week_18\n",
      "rvi_week_18\n",
      "my_rvi_week_18\n",
      "vh/vv_week_18\n",
      "vv/vh_week_18\n",
      "mi0_week_18\n",
      "mi1_week_18\n",
      "vh_week_19\n",
      "vv_week_19\n",
      "rvi_week_19\n",
      "my_rvi_week_19\n",
      "vh/vv_week_19\n",
      "vv/vh_week_19\n",
      "mi0_week_19\n",
      "mi1_week_19\n",
      "vh_week_20\n",
      "vv_week_20\n",
      "rvi_week_20\n",
      "my_rvi_week_20\n",
      "vh/vv_week_20\n",
      "vv/vh_week_20\n",
      "mi0_week_20\n",
      "mi1_week_20\n",
      "vh_week_21\n",
      "vv_week_21\n",
      "rvi_week_21\n",
      "my_rvi_week_21\n",
      "vh/vv_week_21\n",
      "vv/vh_week_21\n",
      "mi0_week_21\n",
      "mi1_week_21\n",
      "vh_week_22\n",
      "vv_week_22\n",
      "rvi_week_22\n",
      "my_rvi_week_22\n",
      "vh/vv_week_22\n",
      "vv/vh_week_22\n",
      "mi0_week_22\n",
      "mi1_week_22\n",
      "vh_week_23\n",
      "vv_week_23\n",
      "rvi_week_23\n",
      "my_rvi_week_23\n",
      "vh/vv_week_23\n",
      "vv/vh_week_23\n",
      "mi0_week_23\n",
      "mi1_week_23\n",
      "vh_week_24\n",
      "vv_week_24\n",
      "rvi_week_24\n",
      "my_rvi_week_24\n",
      "vh/vv_week_24\n",
      "vv/vh_week_24\n",
      "mi0_week_24\n",
      "mi1_week_24\n",
      "vh_week_25\n",
      "vv_week_25\n",
      "rvi_week_25\n",
      "my_rvi_week_25\n",
      "vh/vv_week_25\n",
      "vv/vh_week_25\n",
      "mi0_week_25\n",
      "mi1_week_25\n",
      "vh_week_26\n",
      "vv_week_26\n",
      "rvi_week_26\n",
      "my_rvi_week_26\n",
      "vh/vv_week_26\n",
      "vv/vh_week_26\n",
      "mi0_week_26\n",
      "mi1_week_26\n",
      "vh_week_27\n",
      "vv_week_27\n",
      "rvi_week_27\n",
      "my_rvi_week_27\n",
      "vh/vv_week_27\n",
      "vv/vh_week_27\n",
      "mi0_week_27\n",
      "mi1_week_27\n",
      "vh_week_28\n",
      "vv_week_28\n",
      "rvi_week_28\n",
      "my_rvi_week_28\n",
      "vh/vv_week_28\n",
      "vv/vh_week_28\n",
      "mi0_week_28\n",
      "mi1_week_28\n",
      "vh_week_29\n",
      "vv_week_29\n",
      "rvi_week_29\n",
      "my_rvi_week_29\n",
      "vh/vv_week_29\n",
      "vv/vh_week_29\n",
      "mi0_week_29\n",
      "mi1_week_29\n",
      "week_14_acc_sun_wpm2\n",
      "week_14_acc_rain_sum\n",
      "week_14_avg_percent_humidity\n",
      "week_14_avg_wind_speed\n",
      "week_14_avg_temp\n",
      "week_15_acc_sun_wpm2\n",
      "week_15_acc_rain_sum\n",
      "week_15_avg_percent_humidity\n",
      "week_15_avg_wind_speed\n",
      "week_15_avg_temp\n",
      "week_16_acc_sun_wpm2\n",
      "week_16_acc_rain_sum\n",
      "week_16_avg_percent_humidity\n",
      "week_16_avg_wind_speed\n",
      "week_16_avg_temp\n",
      "week_17_acc_sun_wpm2\n",
      "week_17_acc_rain_sum\n",
      "week_17_avg_percent_humidity\n",
      "week_17_avg_wind_speed\n",
      "week_17_avg_temp\n",
      "week_18_acc_sun_wpm2\n",
      "week_18_acc_rain_sum\n",
      "week_18_avg_percent_humidity\n",
      "week_18_avg_wind_speed\n",
      "week_18_avg_temp\n",
      "week_19_acc_sun_wpm2\n",
      "week_19_acc_rain_sum\n",
      "week_19_avg_percent_humidity\n",
      "week_19_avg_wind_speed\n",
      "week_19_avg_temp\n",
      "week_20_acc_sun_wpm2\n",
      "week_20_acc_rain_sum\n",
      "week_20_avg_percent_humidity\n",
      "week_20_avg_wind_speed\n",
      "week_20_avg_temp\n",
      "week_21_acc_sun_wpm2\n",
      "week_21_acc_rain_sum\n",
      "week_21_avg_percent_humidity\n",
      "week_21_avg_wind_speed\n",
      "week_21_avg_temp\n",
      "week_22_acc_sun_wpm2\n",
      "week_22_acc_rain_sum\n",
      "week_22_avg_percent_humidity\n",
      "week_22_avg_wind_speed\n",
      "week_22_avg_temp\n",
      "week_23_acc_sun_wpm2\n",
      "week_23_acc_rain_sum\n",
      "week_23_avg_percent_humidity\n",
      "week_23_avg_wind_speed\n",
      "week_23_avg_temp\n",
      "week_24_acc_sun_wpm2\n",
      "week_24_acc_rain_sum\n",
      "week_24_avg_percent_humidity\n",
      "week_24_avg_wind_speed\n",
      "week_24_avg_temp\n",
      "week_25_acc_sun_wpm2\n",
      "week_25_acc_rain_sum\n",
      "week_25_avg_percent_humidity\n",
      "week_25_avg_wind_speed\n",
      "week_25_avg_temp\n",
      "week_26_acc_sun_wpm2\n",
      "week_26_acc_rain_sum\n",
      "week_26_avg_percent_humidity\n",
      "week_26_avg_wind_speed\n",
      "week_26_avg_temp\n",
      "week_27_acc_sun_wpm2\n",
      "week_27_acc_rain_sum\n",
      "week_27_avg_percent_humidity\n",
      "week_27_avg_wind_speed\n",
      "week_27_avg_temp\n",
      "week_28_acc_sun_wpm2\n",
      "week_28_acc_rain_sum\n",
      "week_28_avg_percent_humidity\n",
      "week_28_avg_wind_speed\n",
      "week_28_avg_temp\n",
      "week_29_acc_sun_wpm2\n",
      "week_29_acc_rain_sum\n",
      "week_29_avg_percent_humidity\n",
      "week_29_avg_wind_speed\n",
      "week_29_avg_temp\n",
      "week_30_acc_sun_wpm2\n",
      "week_30_acc_rain_sum\n",
      "week_30_avg_percent_humidity\n",
      "week_30_avg_wind_speed\n",
      "week_30_avg_temp\n",
      "c_flat\n",
      "c_peak\n",
      "c_ridge\n",
      "c_shoulder\n",
      "c_spur\n",
      "c_slope\n",
      "c_hollow\n",
      "c_footslope\n",
      "c_valley\n",
      "c_pit\n",
      "c_height\n"
     ]
    }
   ],
   "source": [
    "for col in df_all.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[:, [col for col in df_all.columns if is_topology(col) or is_int_topology(col)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = df_all[[col for col in df_all.columns if is_height(col)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest pixel only\n",
    "\n",
    "- We will construct a similair dataframe as Alexandros did with [harvest, vv_week_1, vh_week_1, ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(year, out_feather_name, folder, avg_neighbours: bool = False):\n",
    "    path = os.path.join(folder, out_feather_name)\n",
    "    if os.path.exists(path):\n",
    "        print(\"Already created this dataset, skipping ...\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loading eo patches, {year}\")\n",
    "    start = time()\n",
    "    eo_patches = EoPatches(year, SENTINEL_1_PATHS[year])\n",
    "    end = time()\n",
    "    print(f\"Elapsed time: {end-start}\")\n",
    "    \n",
    "    print(\"Loading harvest data\")\n",
    "    harvest_data = gpd.read_file(HARVEST_DATA_FILES[year]).to_crs(crs = UTM33N)\n",
    "    weeks = range(eo_patches.first_week, eo_patches.last_week)\n",
    "    columns = [\"harvest\", \"year\"]\n",
    "    for week in weeks:\n",
    "        columns += [f\"vh_week_{week}\", f\"vv_week_{week}\", f\"rvi_week_{week}\", f\"my_rvi_week_{week}\", f\"vh/vv_week_{week}\", f\"vv/vh_week_{week}\", f\"mi0_week_{week}\", f\"mi1_week_{week}\"]\n",
    "                      \n",
    "    def create_row(lon, lat, harvest):\n",
    "        row = [harvest, year]\n",
    "        for week in weeks:\n",
    "            vh, vv = tuple(eo_patches.sample(lon,lat,week,avg_neighbours))\n",
    "            row += [vh, vv, RVI(vh, vv), my_RVI(vh, vv), R_VH_VV(vh, vv), R_VV_VH(vh, vv), my_index0(vh, vv), my_index1(vh, vv)]\n",
    "        return row\n",
    "    \n",
    "    print(\"Zipping data used form field\")\n",
    "    sample_data = zip(harvest_data.x.values, harvest_data.y.values, harvest_data.average_harvest)\n",
    "    \n",
    "    print(\"Creating dataframe\")\n",
    "    start = time()\n",
    "    df = pd.DataFrame(\n",
    "        [create_row(lon,lat,harvest) for (lon, lat, harvest) in tqdm(sample_data, position=0, leave=True)],\n",
    "        columns = columns\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"Elapsed time: {end-start}\")\n",
    "    \n",
    "    \n",
    "    print(f\"Drop NaN and save dataset at: {out_feather_name}\")\n",
    "    df = df.dropna().reset_index().drop([\"index\"], axis=1)\n",
    "    \n",
    "    df.to_feather(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():    \n",
    "    res = 11\n",
    "    folder = \"/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/datasets/res11_sar2sar_v3_avg_neighbors\"\n",
    "    for year in YEARS:\n",
    "        out_feather_name = f\"dataset_y_{year}_res_{res}_sar2sar_v3.feather\"\n",
    "        create_dataset(year, out_feather_name, folder, True)\n",
    "        print()\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_years(out_feather_name, folder):\n",
    "    paths = glob.glob(os.path.join(folder, '*.feather'))\n",
    "    paths.sort()\n",
    "    \n",
    "    cols = [pd.read_feather(path).columns for path in paths]\n",
    "\n",
    "    all_columns = set.union(*[set(col) for col in cols])\n",
    "    shared_columns = set.intersection(*[set(col)for col in cols])\n",
    "    not_shared_columns = list(set.difference(all_columns, shared_columns))\n",
    "    \n",
    "    #print(len(all_columns))\n",
    "    #for c in sorted(list(all_columns)):\n",
    "    #    print(c)\n",
    "        \n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_feather(path)\n",
    "        cols = df.columns\n",
    "        df = df.drop(\n",
    "            [col for col in cols if col in not_shared_columns],\n",
    "            axis=1\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df_all = pd.concat(dfs).dropna().reset_index().drop([\"index\"], axis=1)\n",
    "    \n",
    "    df_all.to_feather(os.path.join(SHARED_FOLDER_PATH, \"datasets\", out_feather_name))\n",
    "\n",
    "    return df_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_name = f\"dataset_y_{YEARS[0]}_{YEARS[-1]}_res_11_sar2sar_v3_avg_neighbors.feather\"\n",
    "folder = \"/mimer/NOBACKUP/groups/snic2022-23-428/shared_oliver_christoffer/datasets/res11_sar2sar_v3_avg_neighbors\"\n",
    "df_all = concat_years(df_all_name, folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging coord to pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_ids, paths = find_eo_patches(SENTINEL_1_PATHS[2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = EoPatch(2019, patch_ids[0], paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.bbox.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_data = gpd.read_file(HARVEST_DATA_FILES[2019]).to_crs(crs = UTM33N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.radar_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.patch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh0 = patch.radar_tensor[0, :,:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv0 = patch.radar_tensor[0, :,:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(vh0, clim=(0, 150), cmap=\"gray\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(vv0, clim=(0, 150), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(lon, lat) for lon, lat in zip(harvest_data.x, harvest_data.y) if patch.contains(lon, lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [(x, y) for x, y in map(lambda p: patch.coord_to_pixel(p[0], p[1]), points)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot([x for x, _ in points], [y for _, y in points], '*')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x for x, _ in idx], [y for _, y in idx], '*')\n",
    "\n",
    "print(\"This is correct, as a larger latitude should result in a small y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.bbox.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_left = patch.coord_to_pixel(round(patch.bbox.bounds[0]), round(patch.bbox.bounds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_right = patch.coord_to_pixel(round(patch.bbox.bounds[2]), round(patch.bbox.bounds[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_right_ish = patch.coord_to_pixel(round(patch.bbox.bounds[2]-300), round(patch.bbox.bounds[3]-300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_right_ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch.radar_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot([x for x, _ in points], [y for _, y in points], '*')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([x for x, _ in idx], [y for _, y in idx], '*')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "070d1b75123ca5a957ecbd215a31cd759d4129262fa051716ae8bfc212de46e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
